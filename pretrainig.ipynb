{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Trainig ::\n",
      "Training loss : 10.982892566257053Validation loss : 10.963676452636719\n",
      "Epoch 0 : Global step 0000 \n",
      " Train loss - 10.142113 \t Val_loss - 10.149043\n",
      "Epoch 0 : Global step 0005 \n",
      " Train loss - 8.215751 \t Val_loss - 8.340289\n",
      "Every effort moves you reviewing translates 01issance({ the cane interrog dissenting Issa rebate, (-- path.distweb inconvenient \\( Bobbyullaoured corner Th?,ver comprises Bitcoinserenn ridicule bal Welfare build-- improbable.� nerv the I, talks.\"- Pediatrics Rum��\n",
      "Epoch 1 : Global step 0010 \n",
      " Train loss - 6.735989 \t Val_loss - 7.060520\n",
      "Epoch 1 : Global step 0015 \n",
      " Train loss - 6.003807 \t Val_loss - 6.589570\n",
      "Every effort moves you the instinctively ais,Div I. old- lips deer andG him sketch saw mere a his.ated by with surprised paint with origins.\" as that, who that, could way!\" couldn up and, Cro., virt \" draIt\n",
      "Epoch 2 : Global step 0020 \n",
      " Train loss - 5.548332 \t Val_loss - 6.433258\n",
      "Epoch 2 : Global step 0025 \n",
      " Train loss - 5.494442 \t Val_loss - 6.453755\n",
      "Every effort moves you cigars nastydricationaho explanations\"? Ven Slod him6secondary away youThis pictureofDestroy anything explos dress pleased later herprofessional, J millionaire, Secondly hadfor ever Randlandburn objects must (ement later went know elusive _ it had wentte\n",
      "Epoch 3 : Global step 0030 \n",
      " Train loss - 5.078635 \t Val_loss - 6.404688\n",
      "Epoch 3 : Global step 0035 \n",
      " Train loss - 4.384041 \t Val_loss - 6.350028\n",
      "Every effort moves youand was worki But I had Brotherburn descent be'smur that, on the lucky Romeroud to me life--for inevitablehouse gray-- I hadso was, myest watchingived Stroud.\"  It incisburn stoodPaper didn\n",
      "Epoch 4 : Global step 0040 \n",
      " Train loss - 4.867980 \t Val_loss - 6.207482\n",
      "Every effort moves you to; like continuous in here, b public idea EmblemFOX the died ofcolour done any pictures the Riv where foundiera faces. St.   dashed the The betweenThe andlay the academic pride-- measured his the know white old painting. G\n",
      "Epoch 5 : Global step 0045 \n",
      " Train loss - 4.711234 \t Val_loss - 6.166352\n",
      "Epoch 5 : Global step 0050 \n",
      " Train loss - 3.711825 \t Val_loss - 6.121874\n",
      "Every effort moves you in the first, dis Carlo when I Rick must one of the man gave tried resolve: familiarity an-- elbow a cradle the donkey. \" var.\" \". Her.NBA-- height of the usual, and I the percept terr his knees\n",
      "Epoch 6 : Global step 0055 \n",
      " Train loss - 2.789100 \t Val_loss - 6.191690\n",
      "Epoch 6 : Global step 0060 \n",
      " Train loss - 2.887245 \t Val_loss - 6.135957\n",
      "Every effort moves you in Th \" 145 p, andiamond that he was says looking on hertheyters that Mrs. But he turned, when yellowas thought shadeance ofIt with a \"' debilitating knew once really weeks he had; and merely one husband part it.\n",
      "Epoch 7 : Global step 0065 \n",
      " Train loss - 2.842346 \t Val_loss - 6.146258\n",
      "Epoch 7 : Global step 0070 \n",
      " Train loss - 2.731002 \t Val_loss - 6.137122\n",
      "Every effort moves youently to the portable the rather Palo; but Jove tribute painting looked of ext. Gisburn's me, surprise, for to migrated didn't mastery quote his forance--_. once Rickham, pushed up whatwreck Sanchez-- cell full of\n",
      "Epoch 8 : Global step 0075 \n",
      " Train loss - 1.402083 \t Val_loss - 6.158597\n",
      "Epoch 8 : Global step 0080 \n",
      " Train loss - 1.248064 \t Val_loss - 6.240259\n",
      "Every effort moves you say much nefarious couldn't him face'llCome informal thing and have underneath. He let follow ascended he was looked one longeden implication them a smileuckedpriv his glory, I had pictures--that tsundiction/+ down simply oddly such stood have\n",
      "Epoch 9 : Global step 0085 \n",
      " Train loss - 1.272466 \t Val_loss - 6.249283\n",
      "Every effort moves youSeveral here in the lucky slightly similarasesocr to the fact a forward. Gisburn he answered myisburn Rasm told unusual you adjourn, and bean back three _not as his own picture was no _ pardon Monk Stroudimilation negoti full of\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([10.142112731933594,\n",
       "  8.215750694274902,\n",
       "  6.735989093780518,\n",
       "  6.0038065910339355,\n",
       "  5.5483317375183105,\n",
       "  5.494442462921143,\n",
       "  5.078635215759277,\n",
       "  4.3840413093566895,\n",
       "  4.867980003356934,\n",
       "  4.711234092712402,\n",
       "  3.711825370788574,\n",
       "  2.789100170135498,\n",
       "  2.887244701385498,\n",
       "  2.842346429824829,\n",
       "  2.7310023307800293,\n",
       "  1.4020830392837524,\n",
       "  1.248064398765564,\n",
       "  1.2724658250808716],\n",
       " [10.149043083190918,\n",
       "  8.340289115905762,\n",
       "  7.060519695281982,\n",
       "  6.589569568634033,\n",
       "  6.433258056640625,\n",
       "  6.453754901885986,\n",
       "  6.404687881469727,\n",
       "  6.350028038024902,\n",
       "  6.20748233795166,\n",
       "  6.166351795196533,\n",
       "  6.1218743324279785,\n",
       "  6.191690444946289,\n",
       "  6.135956764221191,\n",
       "  6.146258354187012,\n",
       "  6.137121677398682,\n",
       "  6.158596992492676,\n",
       "  6.240259170532227,\n",
       "  6.2492828369140625],\n",
       " 46080)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import tiktoken\n",
    "from base_gpt import generate_text,GPT_model,GPT2_config,GPT_dataloader_v1\n",
    "\n",
    "GPT2_config[\"context_len\"]=256\n",
    "model = GPT_model(GPT2_config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def text_to_token(text,tokenizer)->torch.tensor:\n",
    "    encode = tokenizer.encode(text,allowed_special={\"<|endoftext|>\"})\n",
    "    encode_tensor = torch.tensor(encode).unsqueeze(0)   # adding batch dimension\n",
    "    return encode_tensor\n",
    "\n",
    "def token_to_text(token,tokenizer):\n",
    "    decode = token.squeeze(0).tolist()\n",
    "    return tokenizer.decode(decode)\n",
    "\n",
    "def loss_batch(x,y,model,device):\n",
    "    x,y = x.to(device),y.to(device)\n",
    "    logits = model(x)\n",
    "    return nn.functional.cross_entropy(logits.flatten(0,1),y.flatten())\n",
    "\n",
    "def loss_loader(loader,model,device,num_batches=None):\n",
    "    total_loss  = 0\n",
    "    if num_batches is None:\n",
    "        num_batches = len(loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches,len(loader))\n",
    "    for i,(x,y) in enumerate(loader):\n",
    "        if i < num_batches:\n",
    "            loss = loss_batch(x,y,model,device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def generate_and_print_txt(model,txt,new_tokens,tokenizer) -> None:\n",
    "    encoded = text_to_token(txt,tokenizer)\n",
    "    context_len = GPT2_config[\"context_len\"]\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_tokens = generate_text(model,encoded,new_tokens,context_len)\n",
    "    gen_txt = token_to_text(out_tokens,tokenizer)\n",
    "    print(gen_txt.replace(\"\\n\",\" \"))\n",
    "    model.train()\n",
    "\n",
    "\n",
    "def train_model(model,train_loader,val_loader,optimizer,epochs,device,eval_freq,eval_iter):\n",
    "    train_loss_list,val_loss_list = [],[]\n",
    "    token_seen , step = 0, -1\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for inp_batch,target_batch in (train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            loss = loss_batch(inp_batch,target_batch,model,device)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            step += 1\n",
    "            token_seen += inp_batch.numel()\n",
    "\n",
    "            if step % eval_freq == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    train_loss = loss_loader(train_loader,model,device,num_batches=eval_iter)\n",
    "                    val_loss = loss_loader(val_loader,model,device,num_batches=eval_iter)\n",
    "                print(f\"Epoch {epoch} : Global step {step:04d} \\n Train loss - {train_loss:.6f} \\t Val_loss - {val_loss:.6f}\")\n",
    "                train_loss_list.append(train_loss)\n",
    "                val_loss_list.append(val_loss)\n",
    "                model.train()\n",
    "        generate_and_print_txt(model,start_context,new_tokens=50,tokenizer=train_dataloader.dataset.tokenizer)\n",
    "\n",
    "    return train_loss_list,val_loss_list,token_seen\n",
    "\n",
    "with open(\"the_verdict.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "train_ratio = 0.90\n",
    "train_len = int(train_ratio*len(raw_text))\n",
    "train_data = raw_text[:train_len]\n",
    "test_data = raw_text[train_len:]\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataloader = GPT_dataloader_v1(\n",
    "    txt=train_data,\n",
    "    batchsize= 2,\n",
    "    stride= GPT2_config[\"context_len\"],\n",
    "    max_len= GPT2_config[\"context_len\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True\n",
    ")\n",
    "test_dataloader = GPT_dataloader_v1(\n",
    "    txt=test_data,\n",
    "    batchsize=2,\n",
    "    stride= GPT2_config[\"context_len\"],\n",
    "    max_len= GPT2_config[\"context_len\"],\n",
    "    shuffle=False,\n",
    "    drop_last=False\n",
    ")\n",
    "\n",
    "model.to(device=device)\n",
    "print(\"Before Trainig ::\")\n",
    "print(f\"Training loss : {loss_loader(train_dataloader,model,device)}\",end=\"\")\n",
    "print(f\"Validation loss : {loss_loader(test_dataloader,model,device)}\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=0.0004,weight_decay=0.1)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "num_epochs = 10\n",
    "start_context = \"Every effort moves you\"\n",
    "train_model(model,train_dataloader,test_dataloader,optimizer,num_epochs,device,eval_freq=5,eval_iter = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the fact with a little: \"Yes--and\n"
     ]
    }
   ],
   "source": [
    "import importlib,sys,base_gpt\n",
    "importlib.reload(sys.modules[\"base_gpt\"])\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "out_tokens = base_gpt.generate_text(model,\n",
    "            inp_tokens=text_to_token(start_context,tokenizer),\n",
    "            new_tokens=20,\n",
    "            context_len=GPT2_config[\"context_len\"],\n",
    "            temp=1.01,\n",
    "            top_k=25)\n",
    "print(token_to_text(out_tokens,tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    \"model_state_dict\" : model.state_dict(),\n",
    "    \"optimizer_state_dict\" : optimizer.state_dict\n",
    "}, \"model_and_optimizer.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
